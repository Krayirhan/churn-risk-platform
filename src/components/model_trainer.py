# ============================================================================
# model_trainer.py â€” Model EÄŸitimi ve SeÃ§imi BileÅŸeni
# ============================================================================
# NEDEN BU DOSYA VAR?
#   Birden fazla ML modelini eÄŸitip karÅŸÄ±laÅŸtÄ±rÄ±r ve en iyisini seÃ§er.
#   model_params.yaml'daki hiperparametre grid'ini okur,
#   GridSearchCV ile optimum parametreleri bulur.
#
# CHURN PROBLEMÄ°NDE MODEL SEÃ‡Ä°MÄ°:
#   - Accuracy YANILTICI! (%73 "hiÃ§ kimse churn etmez" desen bile %73 accuracy)
#   - F1-score: Precision ve Recall'Ä±n harmonik ortalamasÄ± â†’ dengeli metrik
#   - Recall: Churn edeni yakalamak iÅŸ iÃ§in daha kritik (kaÃ§Ä±rmak = mÃ¼ÅŸteri kaybÄ±)
#   - Bu yÃ¼zden model seÃ§iminde F1 temel alÄ±nÄ±r, Recall da raporlanÄ±r.
#
# DESTEKLENEN MODELLER:
#   1. LogisticRegression â€” Baseline, yorumlanabilir
#   2. RandomForestClassifier â€” Ensemble, feature importance
#   3. XGBClassifier â€” Gradient Boosting, genellikle en iyi performans
#   4. GradientBoostingClassifier â€” sklearn Boosting alternatifi
#
# Ã‡AÄRILIÅ ÅEKLÄ°:
#   train_pipeline.py â†’ ModelTrainer().initiate(X_train, X_test, y_train, y_test)
# ============================================================================

import os
import sys
import numpy as np
from dataclasses import dataclass

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier

from src.exception import CustomException
from src.logger import logging
from src.utils.common import load_yaml, save_object, save_json, evaluate_models


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# KONFÄ°GÃœRASYON
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@dataclass
class ModelTrainerConfig:
    """
    Model eÄŸitim sÃ¼recinin ayarlarÄ±.
    Hangi modeller denenecek, en iyi model nereye kaydedilecek â€” hepsi burada.
    """
    _cfg: dict = None

    def __post_init__(self):
        self._cfg = load_yaml("configs/config.yaml")

        artifacts = self._cfg.get("artifacts", {})
        cv_cfg = self._cfg.get("cv", {})

        # En iyi modelin kaydedileceÄŸi yol
        self.model_path: str = artifacts.get("model_path", "artifacts/model.pkl")

        # KarÅŸÄ±laÅŸtÄ±rma metriklerinin kaydedileceÄŸi yol
        self.metrics_path: str = artifacts.get("metrics_path", "artifacts/metrics.json")

        # Cross-validation ayarlarÄ±
        self.n_folds: int = cv_cfg.get("n_folds", 5)
        self.scoring: str = cv_cfg.get("scoring", "f1")

        # En iyi model kabul eÅŸiÄŸi
        # F1 < 0.5 ise model iÅŸe yaramaz (rastgele tahminden kÃ¶tÃ¼ olabilir)
        self.min_acceptable_f1: float = 0.5


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ANA SINIF
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class ModelTrainer:
    """
    TÃ¼m model eÄŸitim, karÅŸÄ±laÅŸtÄ±rma ve seÃ§im sÃ¼recini yÃ¶neten sÄ±nÄ±f.
    
    KullanÄ±m:
        trainer = ModelTrainer()
        best_f1, report = trainer.initiate(X_train, X_test, y_train, y_test)
    """

    def __init__(self):
        self.config = ModelTrainerConfig()

    def _get_models(self) -> dict:
        """
        Denenecek model nesnelerini dÃ¶ndÃ¼rÃ¼r.
        
        NEDEN SÃ–ZLÃœK?
          - Her modele ismiyle eriÅŸebiliriz.
          - model_params.yaml'daki key'ler bu isimlerle eÅŸleÅŸir.
          - evaluate_models() fonksiyonu bu sÃ¶zlÃ¼ÄŸÃ¼ alÄ±p hepsini dener.
        
        Returns:
            {"model_adÄ±": model_nesnesi, ...}
        """
        models = {
            # --- LOJÄ°STÄ°K REGRESYON ---
            # En basit ve en hÄ±zlÄ± model. Baseline olarak her zaman dahil et.
            # Avantaj: KatsayÄ±lar (coefficients) doÄŸrudan yorumlanabilir.
            # class_weight="balanced" â†’ churn sÄ±nÄ±fÄ±na otomatik aÄŸÄ±rlÄ±k verir.
            "LogisticRegression": LogisticRegression(
                random_state=42,
                class_weight="balanced"
            ),

            # --- RANDOM FOREST ---
            # BirÃ§ok karar aÄŸacÄ±nÄ±n oylamasÄ± (bagging ensemble).
            # Avantaj: Overfitting'e dayanÄ±klÄ±, feature_importances_ saÄŸlar.
            # class_weight="balanced" â†’ her aÄŸaÃ§ta churn aÄŸÄ±rlÄ±ÄŸÄ± artÄ±rÄ±lÄ±r.
            "RandomForestClassifier": RandomForestClassifier(
                random_state=42,
                class_weight="balanced"
            ),

            # --- XGBOOST ---
            # Gradient Boosting'in optimize edilmiÅŸ versiyonu.
            # Genellikle tabular (tablo) verilerinde en iyi performansÄ± verir.
            # scale_pos_weight: churn/no-churn oranÄ± (â‰ˆ2.77)
            # eval_metric="logloss": Binary cross-entropy kaybÄ±
            # use_label_encoder=False: sklearn uyumluluÄŸu iÃ§in
            "XGBClassifier": XGBClassifier(
                random_state=42,
                use_label_encoder=False,
                eval_metric="logloss"
            ),

            # --- GRADIENT BOOSTING (sklearn) ---
            # sklearn'Ä±n kendi gradient boosting implementasyonu.
            # XGBoost kadar hÄ±zlÄ± deÄŸil ama daha stabil olabilir.
            # Not: class_weight doÄŸrudan desteklemez, sample_weight ile halledilir.
            "GradientBoostingClassifier": GradientBoostingClassifier(
                random_state=42
            ),
        }

        return models

    def _get_param_grids(self) -> dict:
        """
        model_params.yaml'dan hiperparametre grid'lerini okur.
        
        NEDEN YAML'DAN?
          - Parametre deÄŸiÅŸtirmek iÃ§in Python koduna dokunmak gerekmez.
          - Yeni parametre denemek â†’ sadece YAML dÃ¼zenle â†’ yeniden Ã§alÄ±ÅŸtÄ±r.
          - Bu, MLOps'un temel prensibi: "config-driven experimentation".
        
        Returns:
            {"model_adÄ±": {"param_name": [value1, value2, ...], ...}, ...}
        """
        try:
            params_cfg = load_yaml("configs/model_params.yaml")
            param_grids = params_cfg.get("models", {})

            logging.info(f"Parametre grid'i yÃ¼klendi. Modeller: {list(param_grids.keys())}")
            return param_grids

        except Exception as e:
            logging.warning(f"model_params.yaml okunamadÄ±, varsayÄ±lan parametrelerle devam ediliyor: {e}")
            return {}

    def initiate(
        self,
        X_train: np.ndarray,
        X_test: np.ndarray,
        y_train: np.ndarray,
        y_test: np.ndarray
    ) -> tuple:
        """
        Model eÄŸitim ve seÃ§im sÃ¼recini baÅŸlatÄ±r.
        
        AKIÅ:
          1. Model sÃ¶zlÃ¼ÄŸÃ¼nÃ¼ ve parametre grid'lerini hazÄ±rla
          2. evaluate_models() ile tÃ¼m modelleri GridSearchCV'de eÄŸit
          3. Test F1-score'a gÃ¶re en iyi modeli seÃ§
          4. En iyi modeli .pkl olarak kaydet
          5. TÃ¼m metrikleri .json olarak kaydet
        
        Args:
            X_train: EÄŸitim feature matrisi (numpy array)
            X_test: Test feature matrisi
            y_train: EÄŸitim hedef vektÃ¶rÃ¼ (0/1)
            y_test: Test hedef vektÃ¶rÃ¼
        
        Returns:
            (best_f1_score, full_report_dict)
        
        Raises:
            CustomException: F1 < min_acceptable_f1 ise
        """
        try:
            logging.info("=" * 60)
            logging.info("MODEL TRAINING baÅŸlatÄ±lÄ±yor...")
            logging.info("=" * 60)

            logging.info(
                f"  Veri boyutlarÄ± â†’ "
                f"X_train: {X_train.shape} | X_test: {X_test.shape} | "
                f"y_train churn oranÄ±: {y_train.mean():.4f}"
            )

            # â”€â”€â”€ ADIM 1: Model ve Parametreleri HazÄ±rla â”€â”€â”€
            models = self._get_models()
            param_grids = self._get_param_grids()

            logging.info(f"  {len(models)} model denenecek: {list(models.keys())}")
            logging.info(f"  CV: {self.config.n_folds}-Fold | Scoring: {self.config.scoring}")

            # â”€â”€â”€ ADIM 2: TÃ¼m Modelleri EÄŸit ve KarÅŸÄ±laÅŸtÄ±r â”€â”€â”€
            # evaluate_models() â†’ common.py'deki fonksiyon
            # Her model iÃ§in GridSearchCV yapar, test metriklerini dÃ¶ndÃ¼rÃ¼r
            report = evaluate_models(
                X_train=X_train,
                y_train=y_train,
                X_test=X_test,
                y_test=y_test,
                models=models,
                params=param_grids,
                cv=self.config.n_folds,
                scoring=self.config.scoring
            )

            # â”€â”€â”€ ADIM 3: En Ä°yi Modeli SeÃ§ â”€â”€â”€
            # TÃ¼m modellerin test F1 score'larÄ±nÄ± topla
            # NEDEN F1?
            #   - Accuracy: %73 "hep No de" bile %73 verir â†’ yanÄ±ltÄ±cÄ±
            #   - F1: Precision ve Recall'Ä±n harmonik ortalamasÄ±
            #   - Churn'de hem yakalama (recall) hem doÄŸruluk (precision) Ã¶nemli
            model_scores = {
                name: metrics["test_f1"]
                for name, metrics in report.items()
            }

            # En yÃ¼ksek F1'e sahip modeli bul
            best_model_name = max(model_scores, key=model_scores.get)
            best_f1 = model_scores[best_model_name]
            best_metrics = report[best_model_name]

            logging.info("")
            logging.info("â”Œ" + "â”€" * 50 + "â”")
            logging.info(f"â”‚  ğŸ† EN Ä°YÄ° MODEL: {best_model_name}")
            logging.info(f"â”‚  F1: {best_f1:.4f} | Recall: {best_metrics['test_recall']:.4f} | "
                        f"Precision: {best_metrics['test_precision']:.4f}")
            logging.info(f"â”‚  AUC: {best_metrics.get('test_roc_auc', 'N/A')}")
            logging.info(f"â”‚  Best params: {best_metrics['best_params']}")
            logging.info("â””" + "â”€" * 50 + "â”˜")

            # â”€â”€â”€ KALÄ°TE KONTROLÃœ â”€â”€â”€
            # F1 eÅŸiÄŸin altÄ±ndaysa dur ve uyar
            if best_f1 < self.config.min_acceptable_f1:
                raise CustomException(
                    f"En iyi model F1={best_f1:.4f} < eÅŸik={self.config.min_acceptable_f1}. "
                    f"Model yeterli performansa ulaÅŸamadÄ±. "
                    f"OlasÄ± Ã§Ã¶zÃ¼mler: daha fazla feature, veri artÄ±rma, farklÄ± modeller.",
                    sys
                )

            # â”€â”€â”€ ADIM 4: En Ä°yi Modeli Kaydet â”€â”€â”€
            # GridSearchCV best_estimator_ zaten refit edilmiÅŸ (tÃ¼m train verisiyle)
            # Bu modeli doÄŸrudan kaydetmek iÃ§in modeli yeniden oluÅŸturmamÄ±z gerekiyor
            best_model_obj = models[best_model_name]
            best_params = best_metrics["best_params"]

            # En iyi parametrelerle yeniden oluÅŸtur ve eÄŸit
            best_model_obj.set_params(**best_params)
            best_model_obj.fit(X_train, y_train)

            save_object(self.config.model_path, best_model_obj)
            logging.info(f"  Model kaydedildi â†’ {self.config.model_path}")

            # â”€â”€â”€ ADIM 5: Metrikleri Kaydet â”€â”€â”€
            # TÃ¼m modellerin karÅŸÄ±laÅŸtÄ±rma raporu + en iyi model bilgisi
            full_report = {
                "best_model": best_model_name,
                "best_f1": best_f1,
                "all_models": report
            }
            save_json(full_report, self.config.metrics_path)
            logging.info(f"  Metrikler kaydedildi â†’ {self.config.metrics_path}")

            logging.info("MODEL TRAINING tamamlandÄ±.")
            logging.info("=" * 60)

            return best_f1, full_report

        except Exception as e:
            raise CustomException(e, sys)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# YARDIMCI: Model KarÅŸÄ±laÅŸtÄ±rma Tablosu YazdÄ±rma
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def print_model_comparison(report: dict) -> None:
    """
    TÃ¼m modellerin metriklerini tablo formatÄ±nda ekrana yazdÄ±rÄ±r.
    
    KullanÄ±m:
        _, report = trainer.initiate(...)
        print_model_comparison(report)
    """
    print("\n" + "=" * 80)
    print("ğŸ“Š MODEL KARÅILAÅTIRMA RAPORU")
    print("=" * 80)
    print(f"{'Model':<30} {'F1':>8} {'Recall':>8} {'Precision':>10} {'AUC':>8} {'Accuracy':>10}")
    print("-" * 80)

    all_models = report.get("all_models", report)
    for name, metrics in all_models.items():
        marker = " ğŸ†" if name == report.get("best_model", "") else ""
        print(
            f"{name:<30} "
            f"{metrics['test_f1']:>8.4f} "
            f"{metrics['test_recall']:>8.4f} "
            f"{metrics['test_precision']:>10.4f} "
            f"{str(metrics.get('test_roc_auc', 'N/A')):>8} "
            f"{metrics['test_accuracy']:>10.4f}"
            f"{marker}"
        )

    print("=" * 80)
    print(f"ğŸ† SeÃ§ilen model: {report.get('best_model', 'N/A')} (F1: {report.get('best_f1', 'N/A')})")
    print()

