# ============================================================================
# prediction_logger.py â€” Tahmin Loglama BileÅŸeni
# ============================================================================
# NEDEN BU DOSYA VAR?
#   Her API tahminini JSONL (JSON Lines) dosyasÄ±na loglar.
#   Bu loglar:
#     1. Data drift analizinin veri kaynaÄŸÄ±
#     2. Model performans izlemenin ground-truth karÅŸÄ±laÅŸtÄ±rma noktasÄ±
#     3. Audit trail (denetim izi) â€” hangi mÃ¼ÅŸteriye ne tahmin yapÄ±ldÄ±?
#     4. A/B test ve retrain kararlarÄ±nÄ±n dayanaÄŸÄ±
#
# FORMAT: JSON Lines (.jsonl)
#   Her satÄ±r baÄŸÄ±msÄ±z bir JSON nesnesi â†’ satÄ±r satÄ±r okunabilir,
#   bÃ¼yÃ¼k dosyalar belleÄŸe sÄ±ÄŸmasa bile stream edilebilir.
#
# KULLANIM:
#   logger = PredictionLogger()
#   logger.log(input_features, prediction_result)
#   recent = logger.get_recent(n=100)  # Son 100 tahmini oku
# ============================================================================

import os
import sys
import json
import glob
import pandas as pd
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from typing import Optional

from src.exception import CustomException
from src.logger import logging
from src.utils.common import load_yaml


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# KONFÄ°GÃœRASYON
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@dataclass
class PredictionLoggerConfig:
    """
    Tahmin loglama ayarlarÄ±. monitoring.yaml'dan okunur.
    """
    _mon: dict = field(default=None, repr=False)

    def __post_init__(self):
        self._mon = load_yaml("configs/monitoring.yaml")
        log_cfg = self._mon.get("prediction_log", {})

        self.enabled: bool = log_cfg.get("enabled", True)
        self.log_dir: str = log_cfg.get("log_dir", "logs/predictions")
        self.file_prefix: str = log_cfg.get("file_prefix", "predictions")
        self.rotation: str = log_cfg.get("rotation", "daily")
        self.max_retention_days: int = log_cfg.get("max_retention_days", 90)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ANA SINIF
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class PredictionLogger:
    """
    Her tahmin sonucunu JSONL dosyasÄ±na loglar.

    NEDEN JSONL?
      - CSV'ye gÃ¶re avantajÄ±: iÃ§ iÃ§e dict'ler desteklenir (input_features).
      - Her satÄ±r baÄŸÄ±msÄ±z parse edilir â†’ bÃ¼yÃ¼k dosyalarda bellek dostu.
      - GÃ¼nlÃ¼k rotasyon: predictions_2026-02-16.jsonl, predictions_2026-02-17.jsonl

    KULLANIM:
        logger = PredictionLogger()
        logger.log(
            input_features={"tenure": 24, "MonthlyCharges": 79.85, ...},
            prediction=1,
            churn_probability=0.82,
            risk_level="YÃ¼ksek",
            customer_id="CUST_001"
        )
    """

    def __init__(self, config: Optional[PredictionLoggerConfig] = None):
        try:
            self.config = config or PredictionLoggerConfig()
            os.makedirs(self.config.log_dir, exist_ok=True)
        except Exception as e:
            raise CustomException(e, sys)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # LOG DOSYA YOLU
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _get_log_path(self, date: Optional[datetime] = None) -> str:
        """
        GÃ¼nÃ¼n tarihine gÃ¶re log dosya yolunu dÃ¶ndÃ¼rÃ¼r.

        ROTASYON:
          daily  â†’ predictions_2026-02-16.jsonl
          Eski dosyalar birikerek drift analizi iÃ§in veri deposu oluÅŸturur.

        Args:
            date: Opsiyonel tarih (None â†’ bugÃ¼n)

        Returns:
            str: Log dosya yolu
        """
        d = date or datetime.now()
        filename = f"{self.config.file_prefix}_{d.strftime('%Y-%m-%d')}.jsonl"
        return os.path.join(self.config.log_dir, filename)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # TAHMÄ°N LOGLA
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def log(
        self,
        input_features: dict,
        prediction: int,
        churn_probability: float,
        risk_level: str,
        customer_id: str = "unknown",
        model_version: str = "v1",
        extra: Optional[dict] = None,
    ) -> str:
        """
        Tek bir tahmin sonucunu JSONL dosyasÄ±na yazar.

        Her Ã§aÄŸrÄ±da dosya aÃ§Ä±lÄ±p tek satÄ±r eklenir (append mode).
        Bu sayede:
          - EÅŸzamanlÄ± istekler dosyayÄ± bozamaz (satÄ±r bazlÄ± yazma)
          - Sunucu Ã§Ã¶kse bile Ã¶nceki loglar kaybolmaz

        Args:
            input_features: MÃ¼ÅŸterinin orijinal feature'larÄ±
            prediction: Model tahmini (0 veya 1)
            churn_probability: Churn olasÄ±lÄ±ÄŸÄ± (0.0-1.0)
            risk_level: Risk seviyesi ("DÃ¼ÅŸÃ¼k", "Orta", "YÃ¼ksek")
            customer_id: MÃ¼ÅŸteri kimliÄŸi
            model_version: Model versiyonu
            extra: Ek metadata (opsiyonel)

        Returns:
            str: YazÄ±lan log dosyasÄ±nÄ±n yolu
        """
        try:
            if not self.config.enabled:
                return ""

            log_entry = {
                "timestamp": datetime.now().isoformat(),
                "customerID": customer_id,
                "prediction": prediction,
                "churn_probability": round(churn_probability, 4),
                "risk_level": risk_level,
                "model_version": model_version,
                "input_features": input_features,
            }

            if extra:
                log_entry["extra"] = extra

            log_path = self._get_log_path()
            with open(log_path, "a", encoding="utf-8") as f:
                f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")

            return log_path

        except Exception as e:
            # Loglama hatasÄ± tahmin akÄ±ÅŸÄ±nÄ± kesmemeli
            logging.error(f"Tahmin loglama hatasÄ±: {e}")
            return ""

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # SON TAHMÄ°NLERÄ° OKU
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def get_recent(self, n: int = 100, days: int = 7) -> pd.DataFrame:
        """
        Son N gÃ¼nÃ¼n tahmin loglarÄ±nÄ± DataFrame olarak dÃ¶ndÃ¼rÃ¼r.

        NEDEN DATAFRAME?
          Drift analizi ve performans izleme pd.DataFrame bekler.
          JSONL'dan satÄ±r satÄ±r okuyup DataFrame'e Ã§eviririz.

        Args:
            n: Maksimum satÄ±r sayÄ±sÄ±
            days: KaÃ§ gÃ¼n geriye git

        Returns:
            pd.DataFrame: Tahmin loglarÄ±
        """
        try:
            all_entries = []

            for day_offset in range(days):
                date = datetime.now() - timedelta(days=day_offset)
                log_path = self._get_log_path(date)

                if not os.path.exists(log_path):
                    continue

                with open(log_path, "r", encoding="utf-8") as f:
                    for line in f:
                        line = line.strip()
                        if line:
                            try:
                                all_entries.append(json.loads(line))
                            except json.JSONDecodeError:
                                continue

            if not all_entries:
                return pd.DataFrame()

            # En son loglar Ã¶nce gelsin
            all_entries.reverse()
            entries = all_entries[:n]

            df = pd.DataFrame(entries)
            logging.info(f"ðŸ“‹ {len(df)} tahmin logu okundu (son {days} gÃ¼n)")
            return df

        except Exception as e:
            logging.error(f"Log okuma hatasÄ±: {e}")
            return pd.DataFrame()

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # FEATURE DATAFRAME â€” Drift Analizi Ä°Ã§in
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def get_features_df(self, n: int = 500, days: int = 7) -> pd.DataFrame:
        """
        Tahmin loglarÄ±ndan input_features sÃ¼tunlarÄ±nÄ± ayrÄ±ÅŸtÄ±rÄ±p
        dÃ¼z DataFrame olarak dÃ¶ndÃ¼rÃ¼r.

        Drift analizi doÄŸrudan bu DataFrame'i kullanÄ±r:
          detector.analyze(logger.get_features_df())

        Args:
            n: Maksimum satÄ±r sayÄ±sÄ±
            days: KaÃ§ gÃ¼n geriye git

        Returns:
            pd.DataFrame: Her satÄ±r bir mÃ¼ÅŸterinin feature'larÄ±
        """
        try:
            recent = self.get_recent(n=n, days=days)
            if recent.empty or "input_features" not in recent.columns:
                return pd.DataFrame()

            features = pd.json_normalize(recent["input_features"])
            logging.info(
                f"ðŸ“Š Feature DataFrame: {features.shape[0]} satÄ±r, "
                f"{features.shape[1]} sÃ¼tun"
            )
            return features

        except Exception as e:
            logging.error(f"Feature extraction hatasÄ±: {e}")
            return pd.DataFrame()

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ESKÄ° LOGLARI TEMÄ°ZLE
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def cleanup_old_logs(self) -> int:
        """
        max_retention_days'den eski log dosyalarÄ±nÄ± siler.

        NEDEN OTOMATÄ°K TEMÄ°ZLÄ°K?
          GÃ¼nlÃ¼k JSONL dosyalarÄ± birikir. 90 gÃ¼nlÃ¼k retention ile
          disk dolmasÄ±nÄ± Ã¶nleriz.

        Returns:
            int: Silinen dosya sayÄ±sÄ±
        """
        try:
            cutoff = datetime.now() - timedelta(days=self.config.max_retention_days)
            pattern = os.path.join(
                self.config.log_dir, f"{self.config.file_prefix}_*.jsonl"
            )
            deleted = 0

            for filepath in glob.glob(pattern):
                filename = os.path.basename(filepath)
                # Dosya adÄ±ndan tarihi parse et: predictions_2026-02-16.jsonl
                try:
                    date_str = filename.replace(
                        f"{self.config.file_prefix}_", ""
                    ).replace(".jsonl", "")
                    file_date = datetime.strptime(date_str, "%Y-%m-%d")
                    if file_date < cutoff:
                        os.remove(filepath)
                        deleted += 1
                except (ValueError, OSError):
                    continue

            if deleted > 0:
                logging.info(f"ðŸ§¹ {deleted} eski log dosyasÄ± silindi")
            return deleted

        except Exception as e:
            logging.error(f"Log temizleme hatasÄ±: {e}")
            return 0

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Ä°STATÄ°STÄ°K Ã–ZETÄ°
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def get_stats(self, days: int = 7) -> dict:
        """
        Son N gÃ¼nÃ¼n tahmin istatistiklerini dÃ¶ndÃ¼rÃ¼r.

        API /monitor/stats endpoint'i bu metodu Ã§aÄŸÄ±rÄ±r.

        Returns:
            dict: {total_predictions, churn_count, churn_rate, avg_probability, ...}
        """
        try:
            recent = self.get_recent(n=10000, days=days)
            if recent.empty:
                return {
                    "total_predictions": 0,
                    "period_days": days,
                    "message": "HenÃ¼z tahmin logu yok",
                }

            total = len(recent)
            churn_count = int((recent["prediction"] == 1).sum())
            churn_rate = round(churn_count / total * 100, 2) if total > 0 else 0.0

            stats = {
                "total_predictions": total,
                "period_days": days,
                "churn_count": churn_count,
                "non_churn_count": total - churn_count,
                "churn_rate": churn_rate,
                "avg_churn_probability": round(
                    float(recent["churn_probability"].mean()), 4
                ),
                "risk_distribution": recent["risk_level"]
                .value_counts()
                .to_dict(),
            }

            return stats

        except Exception as e:
            logging.error(f"Ä°statistik hesaplama hatasÄ±: {e}")
            return {"error": str(e)}
