# ============================================================================
# model_evaluation.py â€” Model DeÄŸerlendirme ve DetaylÄ± Raporlama BileÅŸeni
# ============================================================================
# NEDEN BU DOSYA VAR?
#   model_trainer.py modeli eÄŸitip F1 ile seÃ§er ama detaylÄ± deÄŸerlendirme
#   bu dosyanÄ±n iÅŸi. Trainer "karar verir", Evaluator "raporlar".
#
# NE ÃœRETÄ°R?
#   1. Classification Report: Her sÄ±nÄ±f iÃ§in precision/recall/f1/support
#   2. Confusion Matrix: TP, FP, TN, FN sayÄ±larÄ±
#   3. ROC-AUC ve PR-AUC (Precision-Recall AUC)
#   4. Feature Importance (model destekliyorsa)
#   5. TÃ¼m bunlarÄ± artifacts/metrics.json ve artifacts/confusion_matrix.json'a yazar
#
# NEDEN PR-AUC DA HESAPLANIYOR?
#   Churn verisi dengesiz (%27 churn). Dengesiz veride:
#   - ROC-AUC iyimser olabilir (negatif sÄ±nÄ±f Ã§ok olduÄŸu iÃ§in TN yÃ¼ksek Ã§Ä±kar)
#   - PR-AUC sadece pozitif sÄ±nÄ±fa (Churn=1) odaklanÄ±r â†’ daha dÃ¼rÃ¼st metrik
#
# Ã‡AÄRILIÅ ÅEKLÄ°:
#   train_pipeline.py â†’ ModelEvaluation().initiate(model, X_test, y_test)
# ============================================================================

import os
import sys
import numpy as np
from dataclasses import dataclass

from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    f1_score,
    recall_score,
    precision_score,
    accuracy_score,
    roc_auc_score,
    average_precision_score,  # PR-AUC
    roc_curve,
    precision_recall_curve
)

from src.exception import CustomException
from src.logger import logging
from src.utils.common import load_yaml, load_object, save_json


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# KONFÄ°GÃœRASYON
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@dataclass
class ModelEvaluationConfig:
    """
    DeÄŸerlendirme sÃ¼recinin ayarlarÄ±.
    """
    _cfg: dict = None

    def __post_init__(self):
        self._cfg = load_yaml("configs/config.yaml")

        artifacts = self._cfg.get("artifacts", {})

        # KayÄ±tlÄ± modelin yolu (zaten eÄŸitilmiÅŸ)
        self.model_path: str = artifacts.get("model_path", "artifacts/model.pkl")

        # Metriklerin kaydedileceÄŸi yollar
        self.metrics_path: str = artifacts.get("metrics_path", "artifacts/metrics.json")
        self.confusion_matrix_path: str = artifacts.get(
            "confusion_matrix_path", "artifacts/confusion_matrix.json"
        )

        # Hedef sÃ¼tun ismi (raporda kullanÄ±lacak)
        self.target_col: str = self._cfg.get("target_col", "Churn")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ANA SINIF
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class ModelEvaluation:
    """
    EÄŸitilmiÅŸ modeli test verisi Ã¼zerinde kapsamlÄ± ÅŸekilde deÄŸerlendirir.
    
    ÃœrettiÄŸi Ã§Ä±ktÄ±lar:
      - artifacts/metrics.json: TÃ¼m metrikler
      - artifacts/confusion_matrix.json: Confusion matrix detayÄ±
      - Console'a classification report ve Ã¶zet tablo yazdÄ±rÄ±r
    """

    def __init__(self):
        self.config = ModelEvaluationConfig()

    def _compute_metrics(self, y_true: np.ndarray, y_pred: np.ndarray, y_proba: np.ndarray = None) -> dict:
        """
        TÃ¼m deÄŸerlendirme metriklerini hesaplar.
        
        Args:
            y_true: GerÃ§ek etiketler (0/1)
            y_pred: Tahmin edilen etiketler (0/1)
            y_proba: Pozitif sÄ±nÄ±f olasÄ±lÄ±klarÄ± (0.0-1.0) â€” ROC/PR iÃ§in
        
        Returns:
            dict: TÃ¼m metrikler
        """
        metrics = {}

        # â”€â”€â”€ TEMEL METRÄ°KLER â”€â”€â”€
        
        # ACCURACY: DoÄŸru tahmin oranÄ± (TP + TN) / (TP + TN + FP + FN)
        # âš  Dengesiz veride yanÄ±ltÄ±cÄ±! %73 "hep No de" bile %73 verir.
        metrics["accuracy"] = round(accuracy_score(y_true, y_pred), 4)

        # F1-SCORE: Precision ve Recall'Ä±n harmonik ortalamasÄ±
        # F1 = 2 Ã— (P Ã— R) / (P + R)
        # NEDEN HARMONÄ°K? Aritmetik ortalama P=1, R=0 durumunda 0.5 verir
        # ama harmonik ortalama 0 verir â†’ daha dÃ¼rÃ¼st.
        metrics["f1"] = round(f1_score(y_true, y_pred), 4)

        # RECALL (Sensitivity / TPR): GerÃ§ek churn'lerin ne kadarÄ±nÄ± yakaladÄ±k?
        # Recall = TP / (TP + FN)
        # NEDEN Ã–NEMLÄ°? Churn eden mÃ¼ÅŸteriyi kaÃ§Ä±rmak = gelir kaybÄ±.
        # Recall dÃ¼ÅŸÃ¼kse â†’ churn edecek mÃ¼ÅŸterileri tespit edemiyoruz.
        metrics["recall"] = round(recall_score(y_true, y_pred), 4)

        # PRECISION: "Churn" dediÄŸimiz mÃ¼ÅŸterilerin ne kadarÄ± gerÃ§ekten churn?
        # Precision = TP / (TP + FP)
        # NEDEN Ã–NEMLÄ°? Precision dÃ¼ÅŸÃ¼kse â†’ yanlÄ±ÅŸ alarma maliyet (gereksiz kampanya).
        metrics["precision"] = round(precision_score(y_true, y_pred), 4)

        # â”€â”€â”€ OLABILIRLIK BAZLI METRÄ°KLER â”€â”€â”€
        
        if y_proba is not None:
            # ROC-AUC: ROC eÄŸrisinin altÄ±nda kalan alan
            # 0.5 = rastgele tahmin, 1.0 = mÃ¼kemmel ayrÄ±m
            # EÅŸikten baÄŸÄ±msÄ±z bir metrik â†’ "model ne kadar iyi ayÄ±rt ediyor?"
            metrics["roc_auc"] = round(roc_auc_score(y_true, y_proba), 4)

            # PR-AUC (Average Precision): Precision-Recall eÄŸrisinin altÄ±ndaki alan
            # NEDEN PR-AUC?
            #   Dengesiz veride ROC-AUC iyimser olabilir Ã§Ã¼nkÃ¼ TN Ã§ok yÃ¼ksek.
            #   PR-AUC sadece pozitif sÄ±nÄ±fa odaklanÄ±r â†’ daha gÃ¼venilir.
            #   PR-AUC > 0.5 ise model "ÅŸanstan" iyidir (dengesiz veride).
            metrics["pr_auc"] = round(average_precision_score(y_true, y_proba), 4)

            # ROC eÄŸrisi noktalarÄ± (opsiyonel dashboard iÃ§in)
            fpr, tpr, roc_thresholds = roc_curve(y_true, y_proba)
            metrics["roc_curve"] = {
                "fpr": [round(x, 4) for x in fpr.tolist()],
                "tpr": [round(x, 4) for x in tpr.tolist()],
            }

            # PR eÄŸrisi noktalarÄ±
            pr_precision, pr_recall, pr_thresholds = precision_recall_curve(y_true, y_proba)
            metrics["pr_curve"] = {
                "precision": [round(x, 4) for x in pr_precision.tolist()],
                "recall": [round(x, 4) for x in pr_recall.tolist()],
            }

        return metrics

    def _compute_confusion_matrix(self, y_true: np.ndarray, y_pred: np.ndarray) -> dict:
        """
        Confusion Matrix hesaplar ve yorumlu dict olarak dÃ¶ndÃ¼rÃ¼r.
        
        CONFUSION MATRIX NEDÄ°R?
          Tahmin edilen vs gerÃ§ek etiketlerin 2Ã—2 tablosu:
          
                          Tahmin: No    Tahmin: Yes
          GerÃ§ek: No        TN             FP         â† FP = YanlÄ±ÅŸ Alarm
          GerÃ§ek: Yes       FN             TP         â† FN = KaÃ§Ä±rÄ±lan Churn!
        
        Ä°Å YORUMU:
          - FN (False Negative): Churn edecek mÃ¼ÅŸteriyi "kalmaya devam edecek" dedik.
            BU EN TEHLÄ°KELÄ° HATA! MÃ¼ÅŸteriyi kaybederiz ve Ã¶nlem alamayÄ±z.
          - FP (False Positive): Kalmaya devam edecek mÃ¼ÅŸteriye "churn edecek" dedik.
            Daha az tehlikeli: gereksiz kampanya maliyeti ama mÃ¼ÅŸteriyi kaybetmeyiz.
        
        Returns:
            dict: TN, FP, FN, TP sayÄ±larÄ± ve oranlarÄ±
        """
        cm = confusion_matrix(y_true, y_pred)

        # [[TN, FP],
        #  [FN, TP]]
        tn, fp, fn, tp = cm.ravel()
        total = len(y_true)

        cm_dict = {
            # Ham sayÄ±lar
            "true_negative": int(tn),     # DoÄŸru "No Churn" tahmini
            "false_positive": int(fp),    # YanlÄ±ÅŸ "Churn" alarmÄ±
            "false_negative": int(fn),    # KaÃ§Ä±rÄ±lan churn (TEHLÄ°KELÄ°!)
            "true_positive": int(tp),     # DoÄŸru "Churn" tahmini

            # Oranlar (toplam Ã¼zerinden)
            "tn_rate": round(tn / total, 4),
            "fp_rate": round(fp / total, 4),
            "fn_rate": round(fn / total, 4),
            "tp_rate": round(tp / total, 4),

            # Confusion matrix'in dÃ¼z hali (2D array olarak)
            "matrix": cm.tolist(),

            # Ä°ÅŸ metrikleri
            "total_samples": int(total),
            "total_actual_churn": int(tp + fn),       # GerÃ§ekte churn eden
            "total_actual_no_churn": int(tn + fp),    # GerÃ§ekte kalan
            "total_predicted_churn": int(tp + fp),    # "Churn" dediÄŸimiz
            "total_predicted_no_churn": int(tn + fn), # "No Churn" dediÄŸimiz
        }

        return cm_dict

    def initiate(
        self,
        model=None,
        X_test: np.ndarray = None,
        y_test: np.ndarray = None,
        model_name: str = "best_model"
    ) -> dict:
        """
        Model deÄŸerlendirme sÃ¼recini baÅŸlatÄ±r.
        
        AKIÅ:
          1. Model verilmemiÅŸse â†’ artifacts/model.pkl'den yÃ¼kle
          2. Test verisi Ã¼zerinde tahmin yap (predict + predict_proba)
          3. TÃ¼m metrikleri hesapla (F1, Recall, AUC, PR-AUC vb.)
          4. Confusion matrix hesapla
          5. Classification report yazdÄ±r
          6. SonuÃ§larÄ± JSON olarak kaydet
        
        Args:
            model: EÄŸitilmiÅŸ model nesnesi (None ise diskten yÃ¼klenir)
            X_test: Test feature matrisi
            y_test: Test hedef vektÃ¶rÃ¼
            model_name: Raporda kullanÄ±lacak model ismi
        
        Returns:
            dict: TÃ¼m metrikler ve confusion matrix
        """
        try:
            logging.info("=" * 60)
            logging.info("MODEL EVALUATION baÅŸlatÄ±lÄ±yor...")
            logging.info("=" * 60)

            # â”€â”€â”€ ADIM 1: Modeli YÃ¼kle (gerekirse) â”€â”€â”€
            if model is None:
                logging.info(f"Model diskten yÃ¼kleniyor: {self.config.model_path}")
                model = load_object(self.config.model_path)

            # â”€â”€â”€ ADIM 2: Tahmin Yap â”€â”€â”€
            logging.info(f"Test verisi Ã¼zerinde tahmin yapÄ±lÄ±yor (n={len(y_test)})...")

            # SÄ±nÄ±f tahmini (0 veya 1)
            y_pred = model.predict(X_test)

            # OlasÄ±lÄ±k tahmini (ROC-AUC ve PR-AUC iÃ§in gerekli)
            # predict_proba â†’ [[P(No), P(Yes)], ...] â†’ [:, 1] = P(Yes)
            y_proba = None
            try:
                y_proba = model.predict_proba(X_test)[:, 1]
            except (AttributeError, IndexError):
                logging.warning("âš  Model predict_proba desteklemiyor, olasÄ±lÄ±k bazlÄ± metrikler atlanacak.")

            # â”€â”€â”€ ADIM 3: Metrikleri Hesapla â”€â”€â”€
            logging.info("Metrikler hesaplanÄ±yor...")
            metrics = self._compute_metrics(y_true=y_test, y_pred=y_pred, y_proba=y_proba)
            metrics["model_name"] = model_name

            # â”€â”€â”€ ADIM 4: Confusion Matrix â”€â”€â”€
            logging.info("Confusion matrix hesaplanÄ±yor...")
            cm_dict = self._compute_confusion_matrix(y_true=y_test, y_pred=y_pred)

            # â”€â”€â”€ ADIM 5: Classification Report (konsola yazdÄ±r) â”€â”€â”€
            # sklearn'Ä±n classification_report'u her sÄ±nÄ±f iÃ§in detaylÄ± tablo verir
            cls_report = classification_report(
                y_test, y_pred,
                target_names=["No Churn (0)", "Churn (1)"],
                digits=4
            )
            logging.info(f"\nClassification Report:\n{cls_report}")

            # â”€â”€â”€ ADIM 6: SonuÃ§larÄ± Kaydet â”€â”€â”€
            # Metrikleri JSON'a yaz (dashboard ve karÅŸÄ±laÅŸtÄ±rma iÃ§in)
            eval_result = {
                "model_name": model_name,
                "metrics": {
                    "accuracy": metrics["accuracy"],
                    "f1": metrics["f1"],
                    "recall": metrics["recall"],
                    "precision": metrics["precision"],
                    "roc_auc": metrics.get("roc_auc"),
                    "pr_auc": metrics.get("pr_auc"),
                },
                "confusion_matrix": cm_dict,
            }

            # ROC ve PR eÄŸrilerini ayrÄ± tutabiliriz (bÃ¼yÃ¼k olabilir)
            if "roc_curve" in metrics:
                eval_result["curves"] = {
                    "roc": metrics["roc_curve"],
                    "pr": metrics["pr_curve"],
                }

            save_json(eval_result, self.config.metrics_path)
            save_json(cm_dict, self.config.confusion_matrix_path)

            # â”€â”€â”€ Konsola Ã–zet YazdÄ±r â”€â”€â”€
            self._print_summary(metrics, cm_dict, model_name)

            logging.info("MODEL EVALUATION tamamlandÄ±.")
            logging.info("=" * 60)

            return eval_result

        except Exception as e:
            raise CustomException(e, sys)

    @staticmethod
    def _print_summary(metrics: dict, cm: dict, model_name: str) -> None:
        """
        DeÄŸerlendirme Ã¶zetini gÃ¼zel formatla konsola yazdÄ±rÄ±r.
        """
        print("\n" + "=" * 60)
        print(f"ğŸ“Š MODEL DEÄERLENDÄ°RME RAPORU â€” {model_name}")
        print("=" * 60)

        print(f"\n  ğŸ“ˆ Performans Metrikleri:")
        print(f"     Accuracy    : {metrics['accuracy']:.4f}")
        print(f"     F1-Score    : {metrics['f1']:.4f}")
        print(f"     Recall      : {metrics['recall']:.4f}")
        print(f"     Precision   : {metrics['precision']:.4f}")
        if "roc_auc" in metrics:
            print(f"     ROC-AUC     : {metrics['roc_auc']:.4f}")
        if "pr_auc" in metrics:
            print(f"     PR-AUC      : {metrics['pr_auc']:.4f}")

        print(f"\n  ğŸ“‹ Confusion Matrix:")
        print(f"     {'':>20} Tahmin: No   Tahmin: Yes")
        print(f"     {'GerÃ§ek: No':>20}    {cm['true_negative']:>5}        {cm['false_positive']:>5}")
        print(f"     {'GerÃ§ek: Yes':>20}    {cm['false_negative']:>5}        {cm['true_positive']:>5}")

        print(f"\n  ğŸ’¡ Ä°ÅŸ Yorumu:")
        print(f"     Toplam test verisi       : {cm['total_samples']}")
        print(f"     GerÃ§ek churn sayÄ±sÄ±      : {cm['total_actual_churn']}")
        print(f"     DoÄŸru yakalanan churn    : {cm['true_positive']} "
              f"(Recall: {metrics['recall']:.1%})")
        print(f"     KaÃ§Ä±rÄ±lan churn (FN)     : {cm['false_negative']} "
              f"âš  Bu mÃ¼ÅŸteriler kaybolacak!")
        print(f"     YanlÄ±ÅŸ alarm (FP)        : {cm['false_positive']} "
              f"(gereksiz kampanya maliyeti)")
        print("=" * 60 + "\n")

