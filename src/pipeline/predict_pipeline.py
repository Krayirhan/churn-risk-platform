# ============================================================================
# predict_pipeline.py â€” Tekil MÃ¼ÅŸteri Tahmin Boru HattÄ±
# ============================================================================
# NEDEN BU DOSYA VAR?
#   EÄŸitim tamamlandÄ±ktan sonra yeni bir mÃ¼ÅŸterinin churn olasÄ±lÄ±ÄŸÄ±nÄ±
#   tahmin etmek iÃ§in kullanÄ±lÄ±r. Web API (app.py) ve CLI (main.py)
#   bu dosyayÄ± Ã§aÄŸÄ±rÄ±r.
#
# AKIÅ:
#   KullanÄ±cÄ± JSON gÃ¶nderir â†’ CustomData ile doÄŸrulanÄ±r â†’
#   preprocessor.pkl ile dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼r â†’ model.pkl ile tahmin yapÄ±lÄ±r â†’
#   {churn_probability, risk_level, prediction} dÃ¶ner
#
# Ã–NEMLÄ° KAVRAM â€” TRAIN vs PREDICT PREPROCESSOR:
#   EÄŸitimde fit_transform() yapÄ±ldÄ± â†’ istatistikler (mean, std, encoding haritasÄ±)
#   preprocessor.pkl'ye kaydedildi. Tahmin zamanÄ±nda SADECE transform() yapÄ±lÄ±r.
#   Bu sayede "data leakage" (veri sÄ±zÄ±ntÄ±sÄ±) Ã¶nlenmiÅŸ olur.
#
# Ã‡AÄRILIÅ ÅEKLÄ°:
#   pipeline = PredictPipeline()
#   result = pipeline.predict(customer_data_dict)
#   # â†’ {"prediction": 1, "churn_probability": 0.82, "risk_level": "YÃ¼ksek"}
# ============================================================================

import os
import sys
import numpy as np
import pandas as pd
from dataclasses import dataclass, field, asdict
from typing import Optional

from src.exception import CustomException
from src.logger import logging
from src.utils.common import load_object, load_yaml


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CUSTOM DATA â€” MÃ¼ÅŸteri Verisi DoÄŸrulama ve DÃ¶nÃ¼ÅŸtÃ¼rme
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@dataclass
class CustomData:
    """
    API veya CLI'dan gelen mÃ¼ÅŸteri verisini doÄŸrulayan ve DataFrame'e
    dÃ¶nÃ¼ÅŸtÃ¼ren veri sÄ±nÄ±fÄ±.

    NEDEN @dataclass?
      - Her alanÄ±n tipi ve varsayÄ±lan deÄŸeri aÃ§Ä±kÃ§a tanÄ±mlÄ±dÄ±r.
      - Gelen JSON otomatik olarak bu alanlara map'lenir.
      - Eksik veya hatalÄ± alan kolayca tespit edilir.

    NEDEN VARSAYILAN DEÄERLER?
      - API'den gelen veride bazÄ± alanlar eksik olabilir.
      - Eksik alanlara mantÄ±klÄ± varsayÄ±lan atanÄ±r (preprocessor zaten impute edecek).
      - Bu sayede API esnek kalÄ±r â€” zorunlu alanlar minimum tutulur.

    KullanÄ±m:
        data = CustomData(tenure=24, MonthlyCharges=79.85, Contract="Month-to-month")
        df = data.to_dataframe()
    """

    # â”€â”€â”€ ZORUNLU ALANLAR (varsayÄ±lanÄ± olmayan) â”€â”€â”€
    # Bu Ã¼Ã§ alan churn tahmini iÃ§in en kritik olanlardÄ±r
    tenure: int = 0
    MonthlyCharges: float = 0.0
    TotalCharges: float = 0.0

    # â”€â”€â”€ KATEGORÄ°K ALANLAR â”€â”€â”€
    gender: str = "Male"
    SeniorCitizen: int = 0
    Partner: str = "No"
    Dependents: str = "No"
    PhoneService: str = "Yes"
    MultipleLines: str = "No"
    InternetService: str = "Fiber optic"
    OnlineSecurity: str = "No"
    OnlineBackup: str = "No"
    DeviceProtection: str = "No"
    TechSupport: str = "No"
    StreamingTV: str = "No"
    StreamingMovies: str = "No"
    Contract: str = "Month-to-month"
    PaperlessBilling: str = "Yes"
    PaymentMethod: str = "Electronic check"

    # â”€â”€â”€ KÄ°MLÄ°K (modele girmez ama trace iÃ§in tutulur) â”€â”€â”€
    customerID: str = "PREDICT_USER"

    def to_dataframe(self) -> pd.DataFrame:
        """
        CustomData'yÄ± tek satÄ±rlÄ±k pandas DataFrame'e dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.

        NEDEN DataFrame?
          - preprocessor.pkl bir ColumnTransformer â†’ DataFrame bekler.
          - Tek mÃ¼ÅŸteri bile olsa (1, N) ÅŸeklinde matris olmalÄ±.

        Returns:
            pd.DataFrame: (1, ~20) boyutunda tek satÄ±rlÄ±k DataFrame
        """
        data_dict = asdict(self)
        return pd.DataFrame([data_dict])

    @classmethod
    def from_dict(cls, data: dict) -> "CustomData":
        """
        Dict'ten CustomData oluÅŸturur. Bilinmeyen key'leri sessizce yok sayar.

        NEDEN BU METOD?
          - API'den gelen JSON'da fazladan alanlar olabilir (Ã¶rn: timestamp).
          - @dataclass bunlarÄ± kabul etmez ve hata fÄ±rlatÄ±r.
          - Bu metod sadece tanÄ±mlÄ± alanlarÄ± alÄ±r, kalanÄ±nÄ± yok sayar.

        Args:
            data: MÃ¼ÅŸteri bilgilerini iÃ§eren dict (API body)

        Returns:
            CustomData nesnesi
        """
        valid_fields = {f.name for f in cls.__dataclass_fields__.values()}
        filtered = {k: v for k, v in data.items() if k in valid_fields}
        return cls(**filtered)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# RÄ°SK SEVÄ°YESÄ° SINIFLANDIRMA
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def classify_risk(probability: float) -> str:
    """
    Churn olasÄ±lÄ±ÄŸÄ±nÄ± iÅŸ dÃ¼nyasÄ± iÃ§in anlamlÄ± risk kategorisine Ã§evirir.

    EÅÄ°KLER:
      - < 0.3  â†’ DÃ¼ÅŸÃ¼k   : MÃ¼ÅŸteri muhtemelen kalacak, Ã¶nlem gerekmez
      - < 0.6  â†’ Orta    : Dikkat! Proaktif kampanya dÃ¼ÅŸÃ¼nÃ¼lebilir
      - â‰¥ 0.6  â†’ YÃ¼ksek  : Acil aksiyon! Retention ekibine yÃ¶nlendir

    NEDEN BU EÅÄ°KLER?
      - Telco verisinde churn oranÄ± ~%27. Modelin kalibrasyonuna gÃ¶re
        0.5 eÅŸiÄŸi Ã§oÄŸu zaman fazla agresif olabilir.
      - 3 seviyeli risk sistemi iÅŸ birimlerinin (CRM, pazarlama) anlayacaÄŸÄ±
        dilde sonuÃ§ Ã¼retir.

    Args:
        probability: Model Ã§Ä±ktÄ±sÄ± churn olasÄ±lÄ±ÄŸÄ± (0.0â€“1.0)

    Returns:
        str: "DÃ¼ÅŸÃ¼k", "Orta" veya "YÃ¼ksek"
    """
    if probability < 0.3:
        return "DÃ¼ÅŸÃ¼k"
    elif probability < 0.6:
        return "Orta"
    else:
        return "YÃ¼ksek"


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ANA TAHMÄ°N PIPELINE'I
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class PredictPipeline:
    """
    EÄŸitilmiÅŸ modeli kullanarak tekil mÃ¼ÅŸteri tahmini yapan sÄ±nÄ±f.

    SORUMLULUKLARI:
      1. preprocessor.pkl ve model.pkl'yi diskten yÃ¼kle
      2. Gelen veriyi preprocessor ile dÃ¶nÃ¼ÅŸtÃ¼r (transform â€” fit DEÄÄ°L!)
      3. Model ile tahmin yap (predict + predict_proba)
      4. Sonucu iÅŸ diline Ã§evir (risk seviyesi)

    KullanÄ±m:
        pipeline = PredictPipeline()
        result = pipeline.predict({"tenure": 24, "MonthlyCharges": 79.85, ...})
    """

    def __init__(self):
        self._cfg = load_yaml("configs/config.yaml")
        artifacts = self._cfg.get("artifacts", {})

        self.preprocessor_path: str = artifacts.get(
            "preprocessor_path", "artifacts/preprocessor.pkl"
        )
        self.model_path: str = artifacts.get(
            "model_path", "artifacts/model.pkl"
        )

        # Lazy loading: ilk predict Ã§aÄŸrÄ±sÄ±nda yÃ¼klenir
        self._preprocessor = None
        self._model = None

    def _load_artifacts(self) -> None:
        """
        Model ve preprocessor'Ä± diskten yÃ¼kler (ilk Ã§aÄŸrÄ±da).

        NEDEN LAZY LOADING?
          - Pipeline nesnesi oluÅŸturulduÄŸunda artifact'lar henÃ¼z
            gerekmeyebilir (Ã¶rn: health check endpoint).
          - Ä°lk predict() Ã§aÄŸrÄ±sÄ±nda yÃ¼klenir, sonrakiler bellekten gelir.
          - Bu sayede API baÅŸlatma sÃ¼resi kÄ±salÄ±r.
        """
        if self._preprocessor is None:
            if not os.path.exists(self.preprocessor_path):
                raise FileNotFoundError(
                    f"Preprocessor bulunamadÄ±: {self.preprocessor_path}\n"
                    f"Ã–nce 'python main.py --train' ile eÄŸitim yapÄ±n."
                )
            self._preprocessor = load_object(self.preprocessor_path)

        if self._model is None:
            if not os.path.exists(self.model_path):
                raise FileNotFoundError(
                    f"Model bulunamadÄ±: {self.model_path}\n"
                    f"Ã–nce 'python main.py --train' ile eÄŸitim yapÄ±n."
                )
            self._model = load_object(self.model_path)

    def predict(self, input_data: dict) -> dict:
        """
        Tek bir mÃ¼ÅŸteri iÃ§in churn tahmini yapar.

        AKIÅ:
          1. input_data â†’ CustomData â†’ DataFrame (doÄŸrulama + dÃ¶nÃ¼ÅŸtÃ¼rme)
          2. TelcoCleaner + TelcoFeatureEngineer (CSV modunda eÄŸitildiyse)
          3. preprocessor.transform(df) â†’ numpy array
          4. model.predict() â†’ 0/1 sÄ±nÄ±f tahmini
          5. model.predict_proba() â†’ churn olasÄ±lÄ±ÄŸÄ±
          6. classify_risk() â†’ "DÃ¼ÅŸÃ¼k" / "Orta" / "YÃ¼ksek"

        Ã–NEMLÄ°: NPZ modunda eÄŸitim yapÄ±ldÄ±ysa (Mod 1), notebook'un
        preprocessor'Ä± kullanÄ±ldÄ±ÄŸÄ± iÃ§in burada FE adÄ±mÄ± atlanÄ±r.
        CSV modunda eÄŸitildiyse (Mod 2), FE burada da yapÄ±lmalÄ±dÄ±r.

        Args:
            input_data: MÃ¼ÅŸteri bilgilerini iÃ§eren dict

        Returns:
            dict: {
                "prediction": 0 veya 1,
                "churn_probability": float (0.0â€“1.0),
                "risk_level": "DÃ¼ÅŸÃ¼k" | "Orta" | "YÃ¼ksek",
                "customerID": str
            }
        """
        try:
            logging.info("ğŸ”® Tahmin pipeline baÅŸlatÄ±lÄ±yor...")

            # â”€â”€â”€ 1. Artifact'larÄ± yÃ¼kle â”€â”€â”€
            self._load_artifacts()

            # â”€â”€â”€ 2. Girdi verisini hazÄ±rla â”€â”€â”€
            customer = CustomData.from_dict(input_data)
            df = customer.to_dataframe()
            customer_id = df["customerID"].iloc[0]

            logging.info(f"  MÃ¼ÅŸteri: {customer_id}")
            logging.info(f"  Gelen alanlar: {list(input_data.keys())}")

            # â”€â”€â”€ 3. Feature Engineering (CSV modunda eÄŸitildiyse gerekli) â”€â”€â”€
            # Preprocessor CSV modunda eÄŸitildiyse, FE sÃ¼tunlarÄ± bekliyor olabilir.
            # Bu durumda TelcoCleaner ve TelcoFeatureEngineer'Ä± Ã§alÄ±ÅŸtÄ±rÄ±yoruz.
            # NPZ modunda eÄŸitildiyse preprocessor zaten ham numpy bekler,
            # ama FE yine de zararsÄ±z (fazla sÃ¼tunlar remainder="drop" ile atÄ±lÄ±r).
            try:
                from src.components.data_transformation import (
                    TelcoCleaner,
                    TelcoFeatureEngineer,
                    DataTransformationConfig,
                )

                df = TelcoCleaner.basic_impute(df)
                fe_config = DataTransformationConfig()
                fe = TelcoFeatureEngineer(fe_config)
                df = fe.add_features(df)
                logging.info(f"  FE sonrasÄ± sÃ¼tun sayÄ±sÄ±: {df.shape[1]}")
            except Exception as fe_err:
                logging.warning(
                    f"  âš  Feature Engineering atlandÄ± (NPZ modu olabilir): {fe_err}"
                )

            # â”€â”€â”€ 4. customerID ve Churn sÃ¼tunlarÄ±nÄ± Ã§Ä±kar â”€â”€â”€
            # Preprocessor bu sÃ¼tunlarÄ± tanÄ±maz (remainder="drop" ile atÄ±lmÄ±ÅŸtÄ±)
            cols_to_drop = [c for c in ["customerID", "Churn"] if c in df.columns]
            df_input = df.drop(columns=cols_to_drop)

            # â”€â”€â”€ 5. Preprocessor ile dÃ¶nÃ¼ÅŸtÃ¼r â”€â”€â”€
            # âš  SADECE transform()! Asla fit() yapma â€” train'den Ã¶ÄŸrenilen
            # istatistikler (mean, std, encoding) zaten pkl'de kayÄ±tlÄ±.
            X = self._preprocessor.transform(df_input)

            # Sparse â†’ dense dÃ¶nÃ¼ÅŸÃ¼mÃ¼ (eÄŸer gerekli ise)
            if hasattr(X, "toarray"):
                X = X.toarray()

            # â”€â”€â”€ 6. Model ile tahmin â”€â”€â”€
            prediction = int(self._model.predict(X)[0])

            # OlasÄ±lÄ±k tahmini (model destekliyorsa)
            churn_proba = 0.0
            try:
                proba_arr = self._model.predict_proba(X)
                churn_proba = float(proba_arr[0][1])  # P(Churn=1)
            except (AttributeError, IndexError):
                logging.warning("  âš  Model predict_proba desteklemiyor")
                churn_proba = float(prediction)  # Fallback: 0.0 veya 1.0

            # â”€â”€â”€ 7. Risk seviyesi â”€â”€â”€
            risk = classify_risk(churn_proba)

            result = {
                "prediction": prediction,
                "churn_probability": round(churn_proba, 4),
                "risk_level": risk,
                "customerID": customer_id,
            }

            logging.info(
                f"  âœ… Tahmin: {'CHURN' if prediction == 1 else 'KALACAK'} "
                f"(olasÄ±lÄ±k: {churn_proba:.2%}, risk: {risk})"
            )

            return result

        except Exception as e:
            raise CustomException(e, sys)

    def predict_batch(self, data_list: list[dict]) -> list[dict]:
        """
        Birden fazla mÃ¼ÅŸteri iÃ§in toplu tahmin yapar.

        NEDEN BATCH?
          - API'ye tek seferde 100 mÃ¼ÅŸteri gÃ¶nderilebilir.
          - Model ve preprocessor bir kere yÃ¼klenir, N kere tahmin yapÄ±lÄ±r.
          - Tek tek predict() Ã§aÄŸÄ±rmaktan daha verimli.

        Args:
            data_list: MÃ¼ÅŸteri dict'lerinin listesi

        Returns:
            list[dict]: Her mÃ¼ÅŸteri iÃ§in tahmin sonucu
        """
        try:
            logging.info(f"ğŸ”® Toplu tahmin baÅŸlatÄ±lÄ±yor ({len(data_list)} mÃ¼ÅŸteri)...")
            self._load_artifacts()

            results = []
            for i, customer_data in enumerate(data_list):
                result = self.predict(customer_data)
                results.append(result)

            churn_count = sum(1 for r in results if r["prediction"] == 1)
            logging.info(
                f"  âœ… Toplu tahmin tamamlandÄ±: "
                f"{churn_count}/{len(results)} churn (%{100*churn_count/len(results):.1f})"
            )

            return results

        except Exception as e:
            raise CustomException(e, sys)
